{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149704a-c464-4121-9189-ad29cb8de2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three datasets are created \n",
    "# These Datasets are created after cleaning all the datasets provided \n",
    "# All_PATIENTS.csv This Dataset contains all 25 patients data from HCPA000*P.csv\n",
    "# DEMOGRAPHIC.csv This dataset contains demogrpahic data provided in  \"T1DM_patient_sleep_demographics_with_race.csv\"\n",
    "# PATINETS_WITH_DEMOGRAPHIC.csv This dataset is merge of above two datasets\n",
    "# This Jupyter notebooks has cells:\n",
    "    # importing modules\n",
    "    # declaring variables\n",
    "    # creating a dataframe by merging all csv files having patient data\n",
    "    # change time into date and time and write the clean data into csv file\n",
    "    # dataframe for demographic data by renaming all column names to lowercase with underscore betweeen words and created csv file\n",
    "    # dataframe for merged allpatients and demographic data and created csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b4f00b-1e7b-42e7-a5e8-70602a5cf529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eaa2f031-b490-44ab-8fd8-229b17f4238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "folder_path=\"patients_data\"\n",
    "file_pattern=\"HUPA*.csv\",\n",
    "delimiter=\";\"\n",
    "#Variables\n",
    "#path = \"../HUPA-UC Diabetes Dataset/\"\n",
    "demographic_path = folder_path + \"T1DM_patient_sleep_demographics_with_race.csv\"\n",
    "#all_patients_path = path + \"ALL_PATIENTS.csv\"\n",
    "#modified_demographic_path = path + \"DEMOGRAPHIC.csv\"\n",
    "#patients_demographic_path = path +\"PATIENTS_WITH_DEMOGRAPHIC.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62eb04-083f-4f18-9518-183625063bfc",
   "metadata": {},
   "source": [
    "combined all 25 patient files. cleaned and saved to the final_patients.csv file\n",
    "Removes any repeated rows.\n",
    "Why: Duplicate entries can skew statistics, especially in time-series or patient-level analysis.\n",
    "Drops rows where either time or glucose is missing.\n",
    "Why: These are critical fields for tracking blood sugar trends. Without them, the data point is unusable.\n",
    "Converts the time column to the proper datetime format.\n",
    "Why: Ensures consistency for sorting, filtering, and time-based feature engineering.\n",
    "Note: errors='coerce' turns invalid formats into NaT (missing), which is handled in the next step\n",
    "Removes rows where time couldn't be parsed.\n",
    "Why: Keeps only rows with valid timestamps for chronological analysis.\n",
    "Sorts the data by patient and time, then resets the index.\n",
    "Prepares the dataset for time-series modeling, rolling averages, or lag feature creation per patient.\n",
    "No duplicate or incomplete records\n",
    "Valid timestamps for temporal analysis\n",
    "Chronological ordering per patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02c623-45ec-4e1e-bdb6-c2ad90758e65",
   "metadata": {},
   "source": [
    " Step 1: Load Existing Final File (if it exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10b9ba31-8767-40cf-822c-3cabddb69714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ No existing file found. Starting fresh.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load existing final file\n",
    "final_path = \"final_patients.csv\"\n",
    "if os.path.exists(final_path):\n",
    "    existing_df = pd.read_csv(final_path)\n",
    "    print(f\"‚úÖ Loaded existing file with {len(existing_df)} records\")\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    print(\"üìÇ No existing file found. Starting fresh.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b487e62d-366c-4630-aaa0-470246186dab",
   "metadata": {},
   "source": [
    "Step 2: Load New Patient Files and combine all the 25 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23755f25-e59d-44fc-b6c0-8ab9a67093a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 25 patient files\n",
      "üÜï Loaded new batch with 309392 records\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Load new files\n",
    "folder_path = \"patients_data\"\n",
    "file_pattern = \"HUPA*.csv\"\n",
    "file_list = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "print(f\"üìÅ Found {len(file_list)} patient files\")\n",
    "\n",
    "# Load and tag each file\n",
    "df_list = []\n",
    "for file in file_list:\n",
    "    temp_df = pd.read_csv(file, delimiter=\";\")\n",
    "    temp_df['patient_id'] = os.path.basename(file).split(\".\")[0]\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "# Combine all new files\n",
    "new_df = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"üÜï Loaded new batch with {len(new_df)} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310e50d-e189-4d89-b50c-929f07069580",
   "metadata": {},
   "source": [
    " Step 3: Clean New Data\n",
    "Removes any repeated rows.\n",
    "Why: Duplicate entries can skew statistics, especially in time-series or patient-level analysis.\n",
    "Drops rows where either time or glucose is missing.\n",
    "Why: These are critical fields for tracking blood sugar trends. Without them, the data point is unusable.\n",
    "Converts the time column to the proper datetime format.\n",
    "Why: Ensures consistency for sorting, filtering, and time-based feature engineering.\n",
    "Note: errors='coerce' turns invalid formats into NaT (missing), which is handled in the next step\n",
    "Removes rows where time couldn't be parsed.\n",
    "Why: Keeps only rows with valid timestamps for chronological analysis.\n",
    "Sorts the data by patient and time, then resets the index.\n",
    "Prepares the dataset for time-series modeling, including rolling averages and lag feature creation, per patient.\n",
    "No duplicate or incomplete records\n",
    "Valid timestamps for temporal analysis\n",
    "Chronological ordering per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c95458b2-bb8a-4327-9ed2-cb8a17ec8263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßº Cleaned new data: 309392 records remain\n"
     ]
    }
   ],
   "source": [
    "new_df = new_df.drop_duplicates()\n",
    "new_df = new_df.dropna(subset=['time', 'glucose'])\n",
    "new_df['time'] = pd.to_datetime(new_df['time'], errors='coerce')\n",
    "new_df = new_df.dropna(subset=['time'])\n",
    "new_df = new_df.sort_values(by=['patient_id', 'time']).reset_index(drop=True)\n",
    "print(f\"üßº Cleaned new data: {len(new_df)} records remain\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1bdb57-748c-42ac-b3f9-2f8c6bc269da",
   "metadata": {},
   "source": [
    "Step 4: Merge and Deduplicate\n",
    "Here‚Äôs the critical part ‚Äî we‚Äôll deduplicate based on key columns only:\n",
    "This ensures that even if the same file is reprocessed, it won‚Äôt duplicate rows unless glucose or time differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "21155457-3268-46e4-a6a8-281daffb630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Combined total: 309392 records after deduplication\n"
     ]
    }
   ],
   "source": [
    "# Merge and deduplicate\n",
    "combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "combined_df = combined_df.drop_duplicates(subset=['patient_id', 'time', 'glucose']).reset_index(drop=True)\n",
    "print(f\"üîó Combined total: {len(combined_df)} records after deduplication\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc59099-027e-49dd-8b70-d6a08a8e3b06",
   "metadata": {},
   "source": [
    "Splitting time into date and time helps correlate patient events with demographic factors like \n",
    "age, gender, or region by enabling daily or hourly trend analysis. \n",
    "It also supports clinical insights‚Äîsuch as identifying post-meal glucose spikes, sleep-related vitals, \n",
    "or medication timing effects‚Äîby aligning temporal patterns with medical context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9a0d1f03-d4b1-4690-9dcc-83baa4671111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final reshaped data saved to 'final_patients.csv'\n"
     ]
    }
   ],
   "source": [
    "# Split 'time' into 'date' and 'time_only'\n",
    "combined_df['date'] = combined_df['time'].dt.date\n",
    "combined_df['time_only'] = combined_df['time'].dt.time\n",
    "\n",
    "# Reorder columns: patient_id, date, time, then the rest\n",
    "ordered_cols = ['patient_id', 'date', 'time_only'] + [\n",
    "    col for col in combined_df.columns \n",
    "    if col not in ['patient_id', 'date', 'time_only', 'time']\n",
    "]\n",
    "combined_df = combined_df[ordered_cols]\n",
    "\n",
    "# Rename 'time_only' back to 'time' for clarity\n",
    "combined_df.rename(columns={'time_only': 'time'}, inplace=True)\n",
    "\n",
    "print(\"‚úÖ Final reshaped data saved to 'final_patients.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18472f4-2c2c-445b-b319-1f2814663730",
   "metadata": {},
   "source": [
    "Step-by-Step Precision Cleaning Code\n",
    "This step applies controlled rounding to selected numeric columns to improve readability, reduce noise from excessive precision, \n",
    "and align with clinical reporting standards. It excludes columns that already meet the desired format to avoid redundancy. \n",
    "The goal is to create a clean, interpretable dataset that supports reliable analysis and downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "75790c34-90b0-47b5-b6f8-f138d43377b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_id       date     time  glucose  calories  heart_rate  steps  basal_rate  bolus_volume_delivered  carb_input  calories_cleaned  heart_rate_cleaned  basal_rate_cleaned\n",
      " HUPA0001P 2018-06-13 18:40:00    332.0    6.3595   82.322835   34.0    0.091667                     0.0         0.0              6.36                82.3              0.0917\n",
      " HUPA0001P 2018-06-13 18:45:00    326.0    7.7280   83.740157    0.0    0.091667                     0.0         0.0              7.73                83.7              0.0917\n",
      " HUPA0001P 2018-06-13 18:50:00    330.0    4.7495   80.525180    0.0    0.091667                     0.0         0.0              4.75                80.5              0.0917\n",
      " HUPA0001P 2018-06-13 18:55:00    324.0    6.3595   89.129032   20.0    0.091667                     0.0         0.0              6.36                89.1              0.0917\n",
      " HUPA0001P 2018-06-13 19:00:00    306.0    5.1520   92.495652    0.0    0.075000                     0.0         0.0              5.15                92.5              0.0750\n",
      "['patient_id', 'date', 'time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input', 'calories_cleaned', 'heart_rate_cleaned', 'basal_rate_cleaned']\n"
     ]
    }
   ],
   "source": [
    "# Define only the columns you want to clean\n",
    "columns_to_clean = [\n",
    "    'calories', 'heart_rate', 'basal_rate'\n",
    "]\n",
    "\n",
    "# Precision map for those columns\n",
    "precision_map = {\n",
    "    'calories': 2,\n",
    "    'heart_rate': 1,\n",
    "    'basal_rate': 4,\n",
    "}\n",
    "\n",
    "# Drop all existing _cleaned columns to start fresh\n",
    "combined_df.drop(columns=[col for col in combined_df.columns if col.endswith('_cleaned')], inplace=True)\n",
    "\n",
    "# Apply rounding only to selected columns\n",
    "for col in columns_to_clean:\n",
    "    if col in combined_df.columns and pd.api.types.is_numeric_dtype(combined_df[col]):\n",
    "        combined_df[f\"{col}_cleaned\"] = combined_df[col].round(precision_map[col])\n",
    "\n",
    "print(combined_df.head().to_string(index=False))\n",
    "print(combined_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef66f3-2359-4964-ac81-2d1db752827a",
   "metadata": {},
   "source": [
    " Step 5: Save Final File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "aece9cb9-1d99-4d09-882d-00405d57af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final saved to final_patients.csv\n"
     ]
    }
   ],
   "source": [
    "combined_df.to_csv(final_path, index=False)\n",
    "print(f\"‚úÖ Final saved to {final_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6929f7-01b3-4f54-813f-4cf417401aa3",
   "metadata": {},
   "source": [
    " Step 6: Preview the Final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a0814cf1-be46-49c7-8013-1c63258bebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Final file has 309392 records and 13 columns\n",
      "patient_id       date     time  glucose  calories  heart_rate  steps  basal_rate  bolus_volume_delivered  carb_input  calories_cleaned  heart_rate_cleaned  basal_rate_cleaned\n",
      " HUPA0001P 2018-06-13 18:40:00    332.0    6.3595   82.322835   34.0    0.091667                     0.0         0.0              6.36                82.3              0.0917\n",
      " HUPA0001P 2018-06-13 18:45:00    326.0    7.7280   83.740157    0.0    0.091667                     0.0         0.0              7.73                83.7              0.0917\n",
      " HUPA0001P 2018-06-13 18:50:00    330.0    4.7495   80.525180    0.0    0.091667                     0.0         0.0              4.75                80.5              0.0917\n",
      " HUPA0001P 2018-06-13 18:55:00    324.0    6.3595   89.129032   20.0    0.091667                     0.0         0.0              6.36                89.1              0.0917\n",
      " HUPA0001P 2018-06-13 19:00:00    306.0    5.1520   92.495652    0.0    0.075000                     0.0         0.0              5.15                92.5              0.0750\n"
     ]
    }
   ],
   "source": [
    "# Load and preview the saved final file\n",
    "final_df = pd.read_csv(final_path)\n",
    "\n",
    "# Basic info\n",
    "print(f\"üìä Final file has {len(final_df)} records and {final_df.shape[1]} columns\")\n",
    "\n",
    "# Show first few rows\n",
    "print(final_df.head().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58035748-34a2-483c-b79c-806607e72704",
   "metadata": {},
   "source": [
    "rename the T1DM_patient_sleep_demographics_with_race file column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0b0ad0de-d908-4e8b-b200-501a57af4cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_id  age gender            race  avg_sleep_duration_hrs  sleep_quality_score  sleep_disturbances_percentage\n",
      " HUPA0001P   34   Male           Other                     6.3                  4.5                             80\n",
      " HUPA0002P   49   Male        Hispanic                     6.6                  4.4                             40\n",
      " HUPA0003P   64   Male           Black                     5.3                  5.2                             70\n",
      " HUPA0004P   34 Female Native American                     5.2                  6.9                             60\n",
      " HUPA0005P   49   Male Native American                     5.8                  7.9                             30\n"
     ]
    }
   ],
   "source": [
    "# Load demographics data\n",
    "df_demo = pd.read_csv(os.path.join(\"patients_data\", \"T1DM_patient_sleep_demographics_with_race.csv\"))\n",
    "# renaming column nmaes\n",
    "rename_map = {\n",
    "    'Patient_ID': 'patient_id',\n",
    "    'Age': 'age',\n",
    "    'Gender': 'gender',\n",
    "    'Race': 'race',\n",
    "    'Average Sleep Duration (hrs)': 'avg_sleep_duration_hrs',\n",
    "    'Sleep Quality (1-10)': 'sleep_quality_score',\n",
    "    '% with Sleep Disturbances': 'sleep_disturbances_percentage'\n",
    "}\n",
    "\n",
    "df_demo.rename(columns=rename_map, inplace=True)\n",
    "#saving the data to the cleaned_demographics.csv file \n",
    "df_demo.to_csv(\"cleaned_demographics.csv\", index=False)\n",
    "print(df_demo.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d48b1f-14ff-4ff8-a54e-beb9d92ae678",
   "metadata": {},
   "source": [
    "combining cleaned_demographies and all_patients data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "39664562-662a-4cfe-8b1c-1d1a9fef2d56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Merged Data Summary\n",
      "Total records: 309392\n",
      "Columns: ['patient_id', 'date', 'time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input', 'calories_cleaned', 'heart_rate_cleaned', 'basal_rate_cleaned', 'age', 'gender', 'race', 'avg_sleep_duration_hrs', 'sleep_quality_score', 'sleep_disturbances_percentage']\n",
      "üß™ Sample rows:\n",
      "patient_id       date     time  glucose  calories  heart_rate  steps  basal_rate  bolus_volume_delivered  carb_input  calories_cleaned  heart_rate_cleaned  basal_rate_cleaned  age gender  race  avg_sleep_duration_hrs  sleep_quality_score  sleep_disturbances_percentage\n",
      " HUPA0001P 2018-06-13 18:40:00    332.0    6.3595   82.322835   34.0    0.091667                     0.0         0.0              6.36                82.3              0.0917   34   Male Other                     6.3                  4.5                             80\n",
      " HUPA0001P 2018-06-13 18:45:00    326.0    7.7280   83.740157    0.0    0.091667                     0.0         0.0              7.73                83.7              0.0917   34   Male Other                     6.3                  4.5                             80\n",
      " HUPA0001P 2018-06-13 18:50:00    330.0    4.7495   80.525180    0.0    0.091667                     0.0         0.0              4.75                80.5              0.0917   34   Male Other                     6.3                  4.5                             80\n",
      " HUPA0001P 2018-06-13 18:55:00    324.0    6.3595   89.129032   20.0    0.091667                     0.0         0.0              6.36                89.1              0.0917   34   Male Other                     6.3                  4.5                             80\n",
      " HUPA0001P 2018-06-13 19:00:00    306.0    5.1520   92.495652    0.0    0.075000                     0.0         0.0              5.15                92.5              0.0750   34   Male Other                     6.3                  4.5                             80\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Load final patients data\n",
    "df_patients = pd.read_csv(final_path)\n",
    "\n",
    "# Step 2: Load demographics data\n",
    "df_demograph = pd.read_csv(\"cleaned_demographics.csv\")\n",
    "\n",
    "# Step 3: Merge on patient_id\n",
    "df_merged = pd.merge(df_patients, df_demograph, on=\"patient_id\", how=\"left\")\n",
    "\n",
    "# Step 4: Preview merged data\n",
    "print(\"üîó Merged Data Summary\")\n",
    "print(f\"Total records: {len(df_merged)}\")\n",
    "print(f\"Columns: {df_merged.columns.tolist()}\")\n",
    "print(\"üß™ Sample rows:\")\n",
    "print(df_merged.head().to_string(index=False))\n",
    "\n",
    "# Step 5: Save merged version\n",
    "df_merged.to_csv(\"patients_demography.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ab0ba-0a81-4e67-8e76-cd18b9d7595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply rounding only if current precision exceeds recommended and creates a new column for each one required precission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2d219-7fca-4c12-9c06-7f8512a0c7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
